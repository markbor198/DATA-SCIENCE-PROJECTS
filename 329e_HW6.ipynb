{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "## Mark Borjas (mab7886)\n",
    "\n",
    "## Logistic Regression without using any libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers here if you wish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable inline mode for matplotlib so that Jupyter displays graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your allowed to use only the above libraries that are imported. No other libs should be used in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Dataset \n",
    "\n",
    "In this Assignment we will work with some patients dataset. \n",
    "\n",
    "We have access to 303 patients data. The features are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Thal</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  ExAng  \\\n",
       "0     63    1       typical     145   233    1        2    150      0   \n",
       "1     67    1  asymptomatic     160   286    0        2    108      1   \n",
       "2     67    1  asymptomatic     120   229    0        2    129      1   \n",
       "3     37    1    nonanginal     130   250    0        0    187      0   \n",
       "4     41    0    nontypical     130   204    0        2    172      0   \n",
       "..   ...  ...           ...     ...   ...  ...      ...    ...    ...   \n",
       "298   45    1       typical     110   264    0        0    132      0   \n",
       "299   68    1  asymptomatic     144   193    1        0    141      0   \n",
       "300   57    1  asymptomatic     130   131    0        0    115      1   \n",
       "301   57    0    nontypical     130   236    0        2    174      0   \n",
       "302   38    1    nonanginal     138   175    0        0    173      0   \n",
       "\n",
       "     Oldpeak  Slope   Ca        Thal Target  \n",
       "0        2.3      3  0.0       fixed     No  \n",
       "1        1.5      2  3.0      normal    Yes  \n",
       "2        2.6      2  2.0  reversable    Yes  \n",
       "3        3.5      3  0.0      normal     No  \n",
       "4        1.4      1  0.0      normal     No  \n",
       "..       ...    ...  ...         ...    ...  \n",
       "298      1.2      2  0.0  reversable    Yes  \n",
       "299      3.4      2  2.0  reversable    Yes  \n",
       "300      1.2      2  1.0  reversable    Yes  \n",
       "301      0.0      2  1.0      normal    Yes  \n",
       "302      0.0      1  NaN      normal     No  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "heart_df = pd.read_csv(\"Heart.csv\")\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age:** The person’s age in years\n",
    "\n",
    "**Sex:** The person’s sex (1 = male, 0 = female)\n",
    "\n",
    "**ChestPain:** chest pain type\n",
    "\n",
    "* Value 0: asymptomatic\n",
    "* Value 1: atypical angina\n",
    "* Value 2: non-anginal pain\n",
    "* Value 3: typical angina\n",
    "\n",
    "**RestBP:** The person’s resting blood pressure (mm Hg on admission to the hospital)\n",
    "\n",
    "**Chol:** The person’s cholesterol measurement in mg/dl\n",
    "\n",
    "**Fbs:** The person’s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "restecg: resting electrocardiographic results\n",
    "\n",
    "* Value 0: showing probable or definite left ventricular hypertrophy by Estes’ criteria\n",
    "* Value 1: normal\n",
    "* Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "**RestECG:** The person’s maximum heart rate achieved\n",
    "\n",
    "**MaxHR:** Exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "**Oldpeak:** ST depression induced by exercise relative to rest (‘ST’ relates to positions on the ECG plot. See more here)\n",
    "\n",
    "**Slope:** the slope of the peak exercise ST segment — 0: downsloping; 1: flat; 2: upsloping\n",
    "\n",
    "* 0: downsloping; \n",
    "* 1: flat; \n",
    "* 2: upsloping\n",
    "\n",
    "**Ca:** The number of major vessels (0–3)\n",
    "\n",
    "**Thal:** A blood disorder called thalassemia Value 0: NULL (dropped from the dataset previously\n",
    "\n",
    "* Value 1: fixed defect (no blood flow in some part of the heart)\n",
    "* Value 2: normal blood flow\n",
    "* Value 3: reversible defect (a blood flow is observed but it is not normal)\n",
    "\n",
    "**Target:** Heart disease (1 = no, 0= yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 1 (4 points)\n",
    "We want to use logistic regerssion to perdict if the patients will have heart problems or not. The column \"Target\" in our datasets includes data about heart diseases. If the patient had heart disease we have a 1 and if not a zero. \n",
    "\n",
    "Prepare your data set for predicting heart disease (\"Target\" column) out of 3 features:\n",
    "\n",
    "* Age of the patient (Column **\"Age\"**)\n",
    "* Gender of the patient (male or female - Column **\"Sex\"**)\n",
    "* Cholestrol level of the patient (Column **\"Chol\"**) \n",
    "\n",
    "\n",
    "Split your data into 80% traning data and 20% test data, and implement logistic regression model without using any libs than imported above. \n",
    "\n",
    "* Do maximum **100 iterations**\n",
    "* Use a very small learning rate for checking your GD implementation. \n",
    "* Your are allowed to use your choice of learning rate, like using 0.0001, 0.001 or 0.01 or 0.1 or higher. \n",
    "* Visualize your costs. \n",
    "* No need to add an y-intercept in this task. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 3 # the number of features\n",
    "n_components = 3 # the number of clusters\n",
    "\n",
    "n = 303 # the number of total samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = heart_df[\"Age\"]\n",
    "sex = heart_df[\"Sex\"]\n",
    "chol = heart_df[\"Chol\"]\n",
    "y = heart_df[\"Target\"]\n",
    "#print(len(age),len(sex),(len(chol)))\n",
    "\n",
    "X = np.array([age,sex,chol]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.43894389438944\n",
      "9.023734831198377\n",
      "[60.43557771 43.66369731 36.84487885 49.6359173  58.69452864 62.3949242\n",
      " 60.58696324 51.9672993  45.808519   67.96408253 49.51097821 58.74324613\n",
      " 60.26698146 56.5113462  42.6051069  63.1992984  50.8647443  73.71149934\n",
      " 44.83942754 72.56611443 50.33093567 39.33485131 60.47528746 57.08177999\n",
      " 50.40107406 37.08520469 61.08164441 68.42195553 71.32536097 40.79251806\n",
      " 53.88279528 39.14652206 57.92712556 54.718054   39.7539379  59.91662387\n",
      " 43.97552939 42.20766458 61.1307099  69.15162875 65.54544984 50.01193613\n",
      " 67.04054056 51.32943647 47.48443015 58.07775995 56.7135157  64.08937513\n",
      " 36.62468899 57.68602512 35.88032928 58.10178144 42.56569318 60.39628385\n",
      " 59.21001089 47.22128159 36.22973488 52.28621793 53.98874452 65.39007625\n",
      " 62.29768234 49.03553553 35.7072075  80.22155513 44.04934794 51.44805288\n",
      " 55.35350253 61.28164678 69.01584251 66.84895563 72.86918828 52.11909626\n",
      " 50.8772237  38.26150885 50.8080779  40.46672782 61.76667425 58.64117239\n",
      " 55.80433802 60.27749908 45.11723727 45.69258503 51.95687517 52.73707188\n",
      " 49.11636154 53.69666833 67.17744225 48.32887606 60.49528409 62.91461666\n",
      " 40.7624637  55.26095088 56.58807756 55.99886082 54.83423207 49.29696781\n",
      " 52.59478471 66.46378328 64.41031517 65.45635669 69.81304485 53.6035964\n",
      " 63.53452181 55.37881289 51.06629859 52.42390575 65.41984902 51.77472193\n",
      " 59.42966641 55.28184639 53.09125388 62.07219656 63.37044806 62.18232444\n",
      " 62.01948495 55.1731762  49.94716331 52.12411839 36.56815516 44.64811118\n",
      " 61.42962637 56.00305022 49.27742477 28.99199423 65.33405036 53.04047525\n",
      " 52.26848189 65.62307009 63.76518743 33.4576862  48.49067579 57.61179195\n",
      " 55.86212801 54.78847683 46.46639242 74.61182825 58.71735622 65.50315739\n",
      " 47.95937093 55.37269377 62.41323932 55.97910433 64.71182125 59.63122891\n",
      " 47.89846479 56.04983113 56.11167802 62.26263583 64.57707502 52.7203218\n",
      " 53.86820501 53.49317765 54.20752101 56.71662522 59.12871215 56.26383481\n",
      " 55.94263652 54.10403976 37.11021651 51.71934565 48.15093608 60.7022044\n",
      " 45.31204768 63.32781229 55.37962505 69.46552583 62.06964727 70.89442632\n",
      " 44.71983019 65.99909835 65.17592047 49.8491131  66.93228701 53.53965658\n",
      " 56.04057565 64.85457676 52.33028076 51.91451654 48.57865683 49.35781906\n",
      " 62.88884896 54.75736255 51.21280285 52.24391515 53.11058895 65.16140836\n",
      " 65.08790594 55.95912819 52.32067687 59.00026418 53.31588511 42.24010353\n",
      " 30.20723563 42.33791823 44.65447479 51.63771205 51.46345354 71.19035227\n",
      " 51.4496412  55.4795848  67.70746078 48.56309691 40.1090648  58.00970063\n",
      " 42.51442738 69.01290189 58.38978329 66.47872877 36.08802751 57.96503876\n",
      " 69.94138862 52.786032   45.93487353 46.21494188 56.60941189 60.06456774\n",
      " 37.14270961 48.1458016  50.27887183 35.28569375 43.16252494 63.61337177\n",
      " 48.95159399 44.29976252 64.92529316 57.36837719 63.93662996 37.53286448\n",
      " 59.53636458 67.45308502 61.85016015 44.59791604 55.63742402 56.16457226\n",
      " 64.50560905 60.30459978 68.8599224  67.01174127 48.76018167 62.57636761\n",
      " 58.5422918  44.66722025 53.93460839 76.11351994 53.71471638 53.50799202\n",
      " 47.43387446 59.49296116 58.86328221 58.2185716  54.86254448 51.7060034\n",
      " 54.07294322 75.33525646 40.80038262 55.3752243  55.17007084 48.0755884\n",
      " 69.70975572 58.34328003 66.81014124 64.16007467 42.50116786 59.80647401\n",
      " 55.79314984 60.75964178 67.1530034  61.26732019 45.72886717 44.76552807\n",
      " 57.04590431 45.97104143 55.695049   49.87115888 66.5130454  49.81055896\n",
      " 62.85955467 54.16280246 47.18901052 55.24119267 59.81743658 62.88597315\n",
      " 55.95270585 56.28621865 55.48918071 64.46607652 51.30948136 49.06299639\n",
      " 53.96052327 75.31343669 56.8518165  61.43560223 61.60319982 47.36476519\n",
      " 48.1430856  50.85313151 57.85792115 57.85591849 60.64283472 58.83377209\n",
      " 66.54560505 63.67780201 60.63343483]\n",
      "0.6798679867986799\n",
      "0.4665270703024544\n",
      "246.69306930693068\n",
      "51.69140647264888\n"
     ]
    }
   ],
   "source": [
    "print(np.average(age))\n",
    "print(np.std(age))\n",
    "error1 =np.random.normal(np.average(age), np.std(age), 303)\n",
    "print(error1)\n",
    "print(np.average(sex))\n",
    "print(np.std(sex))\n",
    "print(np.average(chol))\n",
    "print(np.std(chol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-137-0f0ec9e24546>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 0\n",
      "<ipython-input-137-0f0ec9e24546>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[i] = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "298    1\n",
       "299    1\n",
       "300    1\n",
       "301    1\n",
       "302    0\n",
       "Name: Target, Length: 303, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y.values.map(dict(yes=1, no=0))\n",
    "y\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'Yes':\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [ 63   1 233] , y = 0\n",
      "x = [ 67   1 286] , y = 1\n",
      "x = [ 67   1 229] , y = 1\n",
      "x = [ 37   1 250] , y = 0\n",
      "x = [ 41   0 204] , y = 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('x =', X[i, ], ', y =', y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pct_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_pct_index], X[train_pct_index:]\n",
    "y_train, y_test = y[:train_pct_index], y[train_pct_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    To convert continuous value into a range of 0 to 1\n",
    "\n",
    "    I/P\n",
    "    ----------\n",
    "    z : Continuous value\n",
    "\n",
    "    O/P\n",
    "    -------\n",
    "    Value in range between 0 to 1.\n",
    "    \"\"\"\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, weights):\n",
    "    \"\"\"\n",
    "    Compute cost for logistic regression.\n",
    "\n",
    "    I/P\n",
    "    ----------\n",
    "    X : 2D array where each row represent the training example and each column represent the feature ndarray. \n",
    "        Dimension (n x d)\n",
    "        n = number of training examples\n",
    "        d = number of features (including X_0 column of ones)\n",
    "    y : 1D array of labels/target value for each traing example. dimension(1 x n)\n",
    "\n",
    "    weights : 1D array of fitting parameters or weights. Dimension (1 x d)\n",
    "\n",
    "    O/P\n",
    "    -------\n",
    "    cost : The cost of using theta as the parameter for linear regression to fit the data points in X and y.\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    x_dot_weights = X.dot(weights)\n",
    "\n",
    "    cost = 1.0 / n * (-y.T.dot(np.log(sigmoid(x_dot_weights))) - (1 - y).T.dot(np.log(1 - sigmoid(x_dot_weights))))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, weights):\n",
    "    \"\"\"\n",
    "    Compute gradient for logistic regression.\n",
    "\n",
    "    I/P\n",
    "    ----------\n",
    "    X : 2D array where each row represent the training example and each column represent the feature ndarray. \n",
    "    Dimension(m x n)\n",
    "        m = number of training examples\n",
    "        n = number of features (including X_0 column of ones)\n",
    "    y : 1D array of labels/target value for each traing example. dimension(1 x m)\n",
    "\n",
    "    weights : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
    "\n",
    "    O/P\n",
    "    -------\n",
    "    grad: (numpy array)The gradient of the cost with respect to the parameters theta\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    x_dot_weights = X.dot(weights)\n",
    "\n",
    "    grad = (1.0 / n )* (sigmoid(x_dot_weights) - y).T.dot(X)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(n_feature)\n",
    "\n",
    "\n",
    "cost = cost_function(X_train, y_train, weights)\n",
    "grad = gradient(X_train, y_train, weights)\n",
    "\n",
    "#print(cost)\n",
    "#print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0 Cost is:  0.6931471805599462\n",
      "iter:  1 Cost is:  0.6911184331775376\n",
      "iter:  2 Cost is:  0.6900362632202808\n",
      "iter:  3 Cost is:  0.6895345748334204\n",
      "iter:  4 Cost is:  0.6892692145377417\n",
      "iter:  5 Cost is:  0.6891346512728146\n",
      "iter:  6 Cost is:  0.6890589700543243\n",
      "iter:  7 Cost is:  0.6890140977880898\n",
      "iter:  8 Cost is:  0.6889837977631846\n",
      "iter:  9 Cost is:  0.6889609380923366\n",
      "iter:  10 Cost is:  0.6889417173798885\n",
      "iter:  11 Cost is:  0.6889243662375394\n",
      "iter:  12 Cost is:  0.6889079787661341\n",
      "iter:  13 Cost is:  0.6888921208963703\n",
      "iter:  14 Cost is:  0.6888765745596709\n",
      "iter:  15 Cost is:  0.688861233527782\n",
      "iter:  16 Cost is:  0.688846044708142\n",
      "iter:  17 Cost is:  0.6888309817929035\n",
      "iter:  18 Cost is:  0.6888160314433742\n",
      "iter:  19 Cost is:  0.6888011867684373\n",
      "iter:  20 Cost is:  0.6887864440332954\n",
      "iter:  21 Cost is:  0.6887718010604263\n",
      "iter:  22 Cost is:  0.6887572564420875\n",
      "iter:  23 Cost is:  0.688742809149132\n",
      "iter:  24 Cost is:  0.6887284583428362\n",
      "iter:  25 Cost is:  0.6887142032788686\n",
      "iter:  26 Cost is:  0.6887000432626602\n",
      "iter:  27 Cost is:  0.6886859776255672\n",
      "iter:  28 Cost is:  0.6886720057144695\n",
      "iter:  29 Cost is:  0.6886581268857029\n",
      "iter:  30 Cost is:  0.6886443405027215\n",
      "iter:  31 Cost is:  0.6886306459344713\n",
      "iter:  32 Cost is:  0.6886170425549003\n",
      "iter:  33 Cost is:  0.6886035297424766\n",
      "iter:  34 Cost is:  0.6885901068800941\n",
      "iter:  35 Cost is:  0.6885767733549045\n",
      "iter:  36 Cost is:  0.6885635285582928\n",
      "iter:  37 Cost is:  0.6885503718858024\n",
      "iter:  38 Cost is:  0.6885373027371169\n",
      "iter:  39 Cost is:  0.688524320516015\n",
      "iter:  40 Cost is:  0.6885114246303491\n",
      "iter:  41 Cost is:  0.6884986144920111\n",
      "iter:  42 Cost is:  0.6884858895169095\n",
      "iter:  43 Cost is:  0.6884732491249369\n",
      "iter:  44 Cost is:  0.688460692739948\n",
      "iter:  45 Cost is:  0.6884482197897299\n",
      "iter:  46 Cost is:  0.6884358297059737\n",
      "iter:  47 Cost is:  0.6884235219242535\n",
      "iter:  48 Cost is:  0.6884112958839953\n",
      "iter:  49 Cost is:  0.6883991510284533\n",
      "iter:  50 Cost is:  0.688387086804683\n",
      "iter:  51 Cost is:  0.6883751026635164\n",
      "iter:  52 Cost is:  0.6883631980595356\n",
      "iter:  53 Cost is:  0.6883513724510476\n",
      "iter:  54 Cost is:  0.6883396253000599\n",
      "iter:  55 Cost is:  0.6883279560722539\n",
      "iter:  56 Cost is:  0.6883163642369614\n",
      "iter:  57 Cost is:  0.6883048492671396\n",
      "iter:  58 Cost is:  0.6882934106393455\n",
      "iter:  59 Cost is:  0.6882820478337125\n",
      "iter:  60 Cost is:  0.6882707603339265\n",
      "iter:  61 Cost is:  0.6882595476272005\n",
      "iter:  62 Cost is:  0.6882484092042518\n",
      "iter:  63 Cost is:  0.6882373445592772\n",
      "iter:  64 Cost is:  0.6882263531899311\n",
      "iter:  65 Cost is:  0.6882154345972996\n",
      "iter:  66 Cost is:  0.6882045882858794\n",
      "iter:  67 Cost is:  0.6881938137635529\n",
      "iter:  68 Cost is:  0.6881831105415671\n",
      "iter:  69 Cost is:  0.6881724781345091\n",
      "iter:  70 Cost is:  0.6881619160602835\n",
      "iter:  71 Cost is:  0.6881514238400911\n",
      "iter:  72 Cost is:  0.6881410009984049\n",
      "iter:  73 Cost is:  0.6881306470629488\n",
      "iter:  74 Cost is:  0.6881203615646766\n",
      "iter:  75 Cost is:  0.6881101440377471\n",
      "iter:  76 Cost is:  0.6880999940195036\n",
      "iter:  77 Cost is:  0.6880899110504544\n",
      "iter:  78 Cost is:  0.688079894674248\n",
      "iter:  79 Cost is:  0.6880699444376533\n",
      "iter:  80 Cost is:  0.6880600598905386\n",
      "iter:  81 Cost is:  0.6880502405858495\n",
      "iter:  82 Cost is:  0.6880404860795889\n",
      "iter:  83 Cost is:  0.6880307959307953\n",
      "iter:  84 Cost is:  0.6880211697015234\n",
      "iter:  85 Cost is:  0.6880116069568208\n",
      "iter:  86 Cost is:  0.6880021072647118\n",
      "iter:  87 Cost is:  0.6879926701961728\n",
      "iter:  88 Cost is:  0.6879832953251157\n",
      "iter:  89 Cost is:  0.6879739822283648\n",
      "iter:  90 Cost is:  0.6879647304856401\n",
      "iter:  91 Cost is:  0.6879555396795345\n",
      "iter:  92 Cost is:  0.6879464093954959\n",
      "iter:  93 Cost is:  0.687937339221808\n",
      "iter:  94 Cost is:  0.6879283287495693\n",
      "iter:  95 Cost is:  0.687919377572676\n",
      "iter:  96 Cost is:  0.6879104852878001\n",
      "iter:  97 Cost is:  0.6879016514943743\n",
      "iter:  98 Cost is:  0.687892875794569\n",
      "iter:  99 Cost is:  0.6878841577932774\n"
     ]
    }
   ],
   "source": [
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 100\n",
    "learnin_rate = 0.0001\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "\n",
    "# Implementation here is removed. \n",
    "\n",
    "# Your task to implement the GD here. \n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    # \n",
    "    # Calculate the costs \n",
    "    cost = cost_function(X_train, y_train, weights)\n",
    "    \n",
    "    print('iter: ' , i, \"Cost is: \", cost)\n",
    "    # keep the costs for our visualization later \n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    # Calculate the gradients [CODE REMOVED]\n",
    "    grad = gradient(X_train, y_train, weights)\n",
    "    grad = grad.astype('float64')\n",
    "    \n",
    "    # Use the gradient to update the weights [CODE REMOVED] \n",
    "    #print(grad)\n",
    "    weights -= grad * learnin_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_iterations), cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - (4 points)\n",
    "\n",
    "Cacluate the accuracy, Precision, Recall and F1 score of your logistic regression implementaion. \n",
    "Print the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weights, X):\n",
    "    \"\"\"\n",
    "    Predict the class between 0 and 1 using learned logistic regression parameters weights.\n",
    "    Using threshold value 0.5 to convert probability value to class value \n",
    "\n",
    "    I/P\n",
    "    ----------\n",
    "    X : 2D array where each row represent the training example and each column represent the feature ndarray. \n",
    "        Dimension(n x d)\n",
    "        n= number of training examples\n",
    "        d= number of features (including X_0 column of ones)\n",
    "\n",
    "    theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
    "\n",
    "    O/P\n",
    "    -------\n",
    "    Class type based on threshold\n",
    "    \"\"\"\n",
    "    p = sigmoid(X.dot(weights)) >= 0.5\n",
    "    return p.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(weights, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242    0\n",
       "243    1\n",
       "244    0\n",
       "245    1\n",
       "246    1\n",
       "      ..\n",
       "298    1\n",
       "299    1\n",
       "300    1\n",
       "301    1\n",
       "302    0\n",
       "Name: Target, Length: 61, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.47540983606557374\n"
     ]
    }
   ],
   "source": [
    "# Correct Predictions are the cases that are equal. \n",
    "\n",
    "correct = np.sum(predictions == y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy is: \", correct/y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_tn_fn_fp(y_test, predictions):\n",
    "    '''\n",
    "    True positive - actual = 1, predicted = 1\n",
    "    False positive - actual = 1, predicted = 0\n",
    "    False negative - actual = 0, predicted = 1\n",
    "    True negative - actual = 0, predicted = 0\n",
    "    '''\n",
    "    tp = sum((y_test == 1) & (predictions == 1))\n",
    "    tn = sum((y_test == 0) & (predictions == 0))\n",
    "    fn = sum((y_test == 1) & (predictions == 0))\n",
    "    fp = sum((y_test == 0) & (predictions == 1))\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP for Logistic Reg : 1\n",
      "TN for Logistic Reg : 28\n",
      "FP for Logistic Reg : 1\n",
      "FN for Logistic Reg : 31\n"
     ]
    }
   ],
   "source": [
    "tp_lr, tn_lr, fp_lr, fn_lr = compute_tp_tn_fn_fp(y_test, predictions)\n",
    "print('TP for Logistic Reg :', tp_lr)\n",
    "print('TN for Logistic Reg :', tn_lr)\n",
    "print('FP for Logistic Reg :', fp_lr)\n",
    "print('FN for Logistic Reg :', fn_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP for Random Forest : 1\n",
      "TN for Random Forest : 28\n",
      "FP for Random Forest : 1\n",
      "FN for Random Forest : 31\n"
     ]
    }
   ],
   "source": [
    "tp_rf, tn_rf, fp_rf, fn_rf = compute_tp_tn_fn_fp(y_test, predictions)\n",
    "print('TP for Random Forest :', tp_rf)\n",
    "print('TN for Random Forest :', tn_rf)\n",
    "print('FP for Random Forest :', fp_rf)\n",
    "print('FN for Random Forest :', fn_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(tp, fp):\n",
    "    '''\n",
    "    Precision = TP  / FP + TP \n",
    "\n",
    "    '''\n",
    "    return (tp  * 100)/ float( tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Logistic Regression : 50.0\n",
      "Precision for Random Forest : 50.0\n"
     ]
    }
   ],
   "source": [
    "print('Precision for Logistic Regression :', compute_precision(tp_lr, fp_lr))\n",
    "print('Precision for Random Forest :', compute_precision(tp_rf, fp_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(tp, fn):\n",
    "    '''\n",
    "    Recall = TP /FN + TP \n",
    "\n",
    "    '''\n",
    "    return (tp  * 100)/ float( tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Logistic Regression : 3.125\n",
      "Recall for Random Forest : 3.125\n"
     ]
    }
   ],
   "source": [
    "print('Recall for Logistic Regression :', compute_recall(tp_lr, fn_lr))\n",
    "print('Recall for Random Forest :', compute_recall(tp_rf, fn_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(y_true, y_pred):\n",
    "    # calculates the F1 score\n",
    "    tp, tn, fp, fn = compute_tp_tn_fn_fp(y_true, y_pred)\n",
    "    precision = compute_precision(tp, fp)/100\n",
    "    recall = compute_recall(tp, fn)/100\n",
    "    f1_score = (2*precision*recall)/ (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Logistic Regression : 0.058823529411764705\n",
      "F1 score for Random Forest : 0.058823529411764705\n"
     ]
    }
   ],
   "source": [
    "print('F1 score for Logistic Regression :', compute_f1_score(y_test, \n",
    "                                                             predictions))\n",
    "print('F1 score for Random Forest :', compute_f1_score(y_test, \n",
    "                                                             predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - (4 points)\n",
    "\n",
    "\n",
    "Add y-intercept and repeat the above 2 tasks. Do you see any differences after adding the y-intercept? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303,)\n",
      "(303,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "#y = np.expand_dims(y, axis=1)  \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0  Cost= 107.0\n",
      "i= 1  Cost= 68.77270921816626\n",
      "i= 2  Cost= 60.990688806617094\n",
      "i= 3  Cost= 59.40305142013752\n",
      "i= 4  Cost= 59.07572833276637\n",
      "i= 5  Cost= 59.00483732588638\n",
      "i= 6  Cost= 58.98612919952958\n",
      "i= 7  Cost= 58.978047532673436\n",
      "i= 8  Cost= 58.972137264210104\n",
      "i= 9  Cost= 58.966678112787264\n",
      "i= 10  Cost= 58.96132005071154\n",
      "i= 11  Cost= 58.95599184119261\n",
      "i= 12  Cost= 58.950678970088525\n",
      "i= 13  Cost= 58.94537846405268\n",
      "i= 14  Cost= 58.940089697865105\n",
      "i= 15  Cost= 58.93481252409838\n",
      "i= 16  Cost= 58.929546892581044\n",
      "i= 17  Cost= 58.92429277297324\n",
      "i= 18  Cost= 58.9190501390142\n",
      "i= 19  Cost= 58.91381896531724\n",
      "i= 20  Cost= 58.90859922671734\n",
      "i= 21  Cost= 58.90339089813847\n",
      "i= 22  Cost= 58.89819395456651\n",
      "i= 23  Cost= 58.8930083710435\n",
      "i= 24  Cost= 58.8878341226664\n",
      "i= 25  Cost= 58.88267118458702\n",
      "i= 26  Cost= 58.87751953201139\n",
      "i= 27  Cost= 58.87237914020017\n",
      "i= 28  Cost= 58.8672499844681\n",
      "i= 29  Cost= 58.862132040184115\n",
      "i= 30  Cost= 58.85702528277089\n",
      "i= 31  Cost= 58.851929687705514\n",
      "i= 32  Cost= 58.84684523051821\n",
      "i= 33  Cost= 58.84177188679323\n",
      "i= 34  Cost= 58.83670963216829\n",
      "i= 35  Cost= 58.8316584423344\n",
      "i= 36  Cost= 58.82661829303596\n",
      "i= 37  Cost= 58.82158916007057\n",
      "i= 38  Cost= 58.816571019288766\n",
      "i= 39  Cost= 58.8115638465941\n",
      "i= 40  Cost= 58.80656761794294\n",
      "i= 41  Cost= 58.80158230934439\n",
      "i= 42  Cost= 58.79660789686012\n",
      "i= 43  Cost= 58.79164435660419\n",
      "i= 44  Cost= 58.786691664743095\n",
      "i= 45  Cost= 58.78174979749562\n",
      "i= 46  Cost= 58.776818731132664\n",
      "i= 47  Cost= 58.77189844197713\n",
      "i= 48  Cost= 58.766988906403846\n",
      "i= 49  Cost= 58.76209010083927\n",
      "i= 50  Cost= 58.75720200176177\n",
      "i= 51  Cost= 58.752324585701146\n",
      "i= 52  Cost= 58.747457829238726\n",
      "i= 53  Cost= 58.742601709007054\n",
      "i= 54  Cost= 58.737756201690075\n",
      "i= 55  Cost= 58.73292128402268\n",
      "i= 56  Cost= 58.72809693279079\n",
      "i= 57  Cost= 58.72328312483132\n",
      "i= 58  Cost= 58.71847983703184\n",
      "i= 59  Cost= 58.71368704633067\n",
      "i= 60  Cost= 58.70890472971663\n",
      "i= 61  Cost= 58.704132864229024\n",
      "i= 62  Cost= 58.69937142695753\n",
      "i= 63  Cost= 58.69462039504187\n",
      "i= 64  Cost= 58.68987974567207\n",
      "i= 65  Cost= 58.68514945608811\n",
      "i= 66  Cost= 58.680429503579795\n",
      "i= 67  Cost= 58.6757198654868\n",
      "i= 68  Cost= 58.67102051919844\n",
      "i= 69  Cost= 58.6663314421536\n",
      "i= 70  Cost= 58.66165261184053\n",
      "i= 71  Cost= 58.65698400579704\n",
      "i= 72  Cost= 58.65232560160993\n",
      "i= 73  Cost= 58.647677376915404\n",
      "i= 74  Cost= 58.643039309398375\n",
      "i= 75  Cost= 58.638411376793044\n",
      "i= 76  Cost= 58.6337935568821\n",
      "i= 77  Cost= 58.629185827497075\n",
      "i= 78  Cost= 58.62458816651809\n",
      "i= 79  Cost= 58.62000055187372\n",
      "i= 80  Cost= 58.61542296154101\n",
      "i= 81  Cost= 58.61085537354518\n",
      "i= 82  Cost= 58.60629776595968\n",
      "i= 83  Cost= 58.60175011690605\n",
      "i= 84  Cost= 58.59721240455365\n",
      "i= 85  Cost= 58.59268460711997\n",
      "i= 86  Cost= 58.58816670286989\n",
      "i= 87  Cost= 58.58365867011623\n",
      "i= 88  Cost= 58.579160487219376\n",
      "i= 89  Cost= 58.57467213258676\n",
      "i= 90  Cost= 58.57019358467359\n",
      "i= 91  Cost= 58.565724821982094\n",
      "i= 92  Cost= 58.56126582306164\n",
      "i= 93  Cost= 58.55681656650864\n",
      "i= 94  Cost= 58.552377030966476\n",
      "i= 95  Cost= 58.54794719512525\n",
      "i= 96  Cost= 58.54352703772178\n",
      "i= 97  Cost= 58.53911653753966\n",
      "i= 98  Cost= 58.53471567340875\n",
      "i= 99  Cost= 58.5303244242055\n"
     ]
    }
   ],
   "source": [
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 100\n",
    "learningRate = 0.00001\n",
    "b_current = 0 \n",
    "m_current = 0\n",
    "cost_list = []\n",
    "\n",
    "beta = np.zeros(3)\n",
    "size = int(n)\n",
    "\n",
    "# Implementation here is removed. \n",
    "\n",
    "# Your task to implement the GD here. \n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Calculate the prediction with current regression coefficients. \n",
    "    cost = 0\n",
    "    m_gradient = 0\n",
    "    b_gradient = 0\n",
    "    \n",
    "    for j in range(242):\n",
    "        \n",
    "        y_prediction = np.dot(beta, X_train[j]) + b_current \n",
    "    \n",
    "        # We compute costs just for monitoring \n",
    "        cost += ( y_train[j] - y_prediction)**2\n",
    "\n",
    "        # calculate gradients.\n",
    "        m_gradient += X_train[j] * (y_train[j] - y_prediction)\n",
    "        b_gradient += ( y_train[j] - y_prediction)\n",
    "    \n",
    "    m_gradient = (-1.0/n)* m_gradient\n",
    "    b_gradient = (-1.0/n)* b_gradient\n",
    "    \n",
    "    #print(\"i=\", i ,\" m = \", m_current, \" b=\", b_current, \" Cost=\", cost) \n",
    "    print(\"i=\", i , \" Cost=\", cost)  \n",
    "    cost_list.append(cost)\n",
    "    # update the weights - Regression Coefficients \n",
    "    #beta = m_current\n",
    "    m_current = m_current - learningRate * m_gradient\n",
    "    beta = m_current\n",
    "    b_current = b_current - learningRate * b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_iterations), cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weights, X):\n",
    "    \"\"\"\n",
    "    Predict the class between 0 and 1 using learned logistic regression parameters weights.\n",
    "    Using threshold value 0.5 to convert probability value to class value \n",
    "\n",
    "    I/P\n",
    "    ----------\n",
    "    X : 2D array where each row represent the training example and each column represent the feature ndarray. \n",
    "        Dimension(n x d)\n",
    "        n= number of training examples\n",
    "        d= number of features (including X_0 column of ones)\n",
    "\n",
    "    theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
    "\n",
    "    O/P\n",
    "    -------\n",
    "    Class type based on threshold\n",
    "    \"\"\"\n",
    "    p = sigmoid(X.dot(weights)) >= 0.5\n",
    "    return p.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predict(weights, X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.47540983606557374\n"
     ]
    }
   ],
   "source": [
    "# Correct Predictions are the cases that are equal. \n",
    "\n",
    "correct = np.sum(predictions == y_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy is: \", correct/y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tp_tn_fn_fp(y_test, predictions):\n",
    "    '''\n",
    "    True positive - actual = 1, predicted = 1\n",
    "    False positive - actual = 1, predicted = 0\n",
    "    False negative - actual = 0, predicted = 1\n",
    "    True negative - actual = 0, predicted = 0\n",
    "    '''\n",
    "    tp = sum((y_test == 1) & (predictions == 1))\n",
    "    tn = sum((y_test == 0) & (predictions == 0))\n",
    "    fn = sum((y_test == 1) & (predictions == 0))\n",
    "    fp = sum((y_test == 0) & (predictions == 1))\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP for Logistic Reg : 1\n",
      "TN for Logistic Reg : 28\n",
      "FP for Logistic Reg : 1\n",
      "FN for Logistic Reg : 31\n"
     ]
    }
   ],
   "source": [
    "tp_lr, tn_lr, fp_lr, fn_lr = compute_tp_tn_fn_fp(y_test, predictions)\n",
    "print('TP for Logistic Reg :', tp_lr)\n",
    "print('TN for Logistic Reg :', tn_lr)\n",
    "print('FP for Logistic Reg :', fp_lr)\n",
    "print('FN for Logistic Reg :', fn_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP for Random Forest : 1\n",
      "TN for Random Forest : 28\n",
      "FP for Random Forest : 1\n",
      "FN for Random Forest : 31\n"
     ]
    }
   ],
   "source": [
    "tp_rf, tn_rf, fp_rf, fn_rf = compute_tp_tn_fn_fp(y_test, predictions)\n",
    "print('TP for Random Forest :', tp_rf)\n",
    "print('TN for Random Forest :', tn_rf)\n",
    "print('FP for Random Forest :', fp_rf)\n",
    "print('FN for Random Forest :', fn_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(tp, fp):\n",
    "    '''\n",
    "    Precision = TP  / FP + TP \n",
    "\n",
    "    '''\n",
    "    return (tp  * 100)/ float( tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Logistic Regression : 50.0\n",
      "Precision for Random Forest : 50.0\n"
     ]
    }
   ],
   "source": [
    "print('Precision for Logistic Regression :', compute_precision(tp_lr, fp_lr))\n",
    "print('Precision for Random Forest :', compute_precision(tp_rf, fp_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(tp, fn):\n",
    "    '''\n",
    "    Recall = TP /FN + TP \n",
    "\n",
    "    '''\n",
    "    return (tp  * 100)/ float( tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for Logistic Regression : 3.125\n",
      "Recall for Random Forest : 3.125\n"
     ]
    }
   ],
   "source": [
    "print('Recall for Logistic Regression :', compute_recall(tp_lr, fn_lr))\n",
    "print('Recall for Random Forest :', compute_recall(tp_rf, fn_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(y_true, y_pred):\n",
    "    # calculates the F1 score\n",
    "    tp, tn, fp, fn = compute_tp_tn_fn_fp(y_true, y_pred)\n",
    "    precision = compute_precision(tp, fp)/100\n",
    "    recall = compute_recall(tp, fn)/100\n",
    "    f1_score = (2*precision*recall)/ (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Logistic Regression : 0.058823529411764705\n",
      "F1 score for Random Forest : 0.058823529411764705\n"
     ]
    }
   ],
   "source": [
    "print('F1 score for Logistic Regression :', compute_f1_score(y_test, \n",
    "                                                             predictions))\n",
    "print('F1 score for Random Forest :', compute_f1_score(y_test, \n",
    "                                                             predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Implement the Bold Driver   - (4 points)\n",
    "In your GD implementation, add the bold driver idea to have a dynamic learning rate.\n",
    "\n",
    "* Add a stop codition to stop the GD when the cost is not changing more than 0.001. (differences between two costs not more than 0.001, then stop)\n",
    "* Can you stop earlier than 100 iterations? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(n_feature)\n",
    "\n",
    "\n",
    "cost = cost_function(X_train, y_train, weights)\n",
    "grad = gradient(X_train, y_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0 Cost is:  0.6892692145377417\n",
      "iter:  1 Cost is:  0.6891346512728146\n",
      "Stoped at iteration 1\n"
     ]
    }
   ],
   "source": [
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 100\n",
    "learnin_rate = 0.0001\n",
    "\n",
    "cost_list = []\n",
    "oldCost=0\n",
    "precision = 0.001\n",
    "\n",
    "\n",
    "# Implementation here is removed. \n",
    "\n",
    "# Your task to implement the GD here. \n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    # \n",
    "    # Calculate the costs \n",
    "    cost = cost_function(X_train, y_train, weights)\n",
    "    \n",
    "    print('iter: ' , i, \"Cost is: \", cost)\n",
    "    # keep the costs for our visualization later \n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    # Calculate the gradients [CODE REMOVED]\n",
    "    grad = gradient(X_train, y_train, weights)\n",
    "    grad = grad.astype('float64')\n",
    "    \n",
    "    # Use the gradient to update the weights [CODE REMOVED] \n",
    "    #print(grad)\n",
    "    weights -= grad * learnin_rate\n",
    "    if(abs(cost - oldCost) <= precision):\n",
    "        print(\"Stoped at iteration\", i)\n",
    "        break\n",
    "    \n",
    "    oldCost = cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Implement the L2 norm regularization.  - (4 points)\n",
    "\n",
    "Modify your Cost and gradient to implement the l2 norm regularization. \n",
    "Repreat the task 1 and 2 to check if your result is changing. \n",
    "\n",
    "* Use y-itercept. \n",
    "* Do max 100 iterations as before and report your accuracy, Precision, Recall and F1. \n",
    "* You can stop earlier when the cost is not changing than 0.001. \n",
    "\n",
    "\n",
    "**Optional:** you might want to use the bold driver. But you can do this task without the bold driver as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(X,y,theta,lamda = 10):\n",
    "    '''Cost function for ridge regression (regularized L2)'''\n",
    "    #Initialization\n",
    "    m = len(y) \n",
    "    J = 0\n",
    "    \n",
    "    #Vectorized implementation\n",
    "    h = X @ theta\n",
    "    J_reg = (lamda / (2*m)) * np.sum(np.square(theta))\n",
    "    J = float((1./(2*m)) * (h - y).T @ (h - y)) + J_reg;\n",
    "    return(J)\n",
    "\n",
    "def gradient_descent_reg(X,y,theta,alpha = 0.0005,lamda = 10,num_iters=100):\n",
    "    '''Gradient descent for ridge regression'''\n",
    "    #Initialisation of useful values \n",
    "    m = np.size(y)\n",
    "    J_history = np.zeros(num_iters)\n",
    "    theta_0_hist, theta_1_hist = [], [] #Used for three D plot\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        #Hypothesis function\n",
    "        h = np.dot(X,theta)\n",
    "        \n",
    "        #Grad function in vectorized form\n",
    "        theta = theta - alpha * (1/m)* (  (X.T @ (h-y)) + lamda * theta )\n",
    "           \n",
    "        #Cost function in vectorized form       \n",
    "        J_history[i] = costFunctionReg(X,y,theta,lamda)\n",
    "           \n",
    "        #Calculate the cost for each iteration(used to plot convergence)\n",
    "        #theta_0_hist.append(theta[0])\n",
    "        #theta_1_hist.append(theta[1])   \n",
    "    return theta, J_history\n",
    "         \n",
    "def closed_form_reg_solution(X,y,lamda = 10): \n",
    "    '''Closed form solution for ridge regression'''\n",
    "    m,n = X.shape\n",
    "    I = np.eye((n))\n",
    "    return (np.linalg.inv(X.T @ X + lamda * I) @ X.T @ y)\n",
    "\n",
    "def cost_l2(x,y):\n",
    "    return x**2 + y**2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([nan, nan, nan], dtype=object), array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "       nan, nan, nan, nan, nan, nan, nan, nan, nan]))\n"
     ]
    }
   ],
   "source": [
    "# Now we optimize it using Gradient Descent. \n",
    "num_iterations = 100\n",
    "learnin_rate = 0.0001\n",
    "cost_list = []\n",
    "oldCost=0\n",
    "\n",
    "#weights = np.zeros(n_feature)\n",
    "T1, T2 = np.meshgrid(np.linspace(-3,3,30),np.linspace(-3,3,30))\n",
    "# Implementation here is removed. \n",
    "\n",
    "# Your task to implement the GD here. \n",
    "grad = gradient_descent_reg(X, y, weights,0.0001,3,num_iters=100)\n",
    "\n",
    "#for i in range(0, num_iterations):\n",
    "    \n",
    "    # \n",
    "    # Calculate the costs \n",
    "    #cost = costFunctionReg(X_train, y_train,weights)\n",
    "    \n",
    "#print('iter: ' , i, \"Cost is: \", grad)\n",
    "    # keep the costs for our visualization later \n",
    "    \n",
    "    #cost_list.append(cost)\n",
    "    \n",
    "    # Calculate the gradients [CODE REMOVED]\n",
    "    #grad = gradient_descent_reg(X_train, y_train, weights.reshape(3),num_iters=100)\n",
    "    #grad = grad.astype('float64')\n",
    "    \n",
    "    # Use the gradient to update the weights [CODE REMOVED] \n",
    "print(grad)\n",
    "    #weights -= grad * learnin_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005598430989638045 0.25621148806547056 -3.3121290991719575e-05]\n"
     ]
    }
   ],
   "source": [
    "print(closed_form_reg_solution(X,y,lamda = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
